{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "<h2>Introduction</h2>\n",
    "<p>\n",
    "    Today we have so many laptops in the market<br>\n",
    "    All with different prices, brands, graphic cards, hard drives types and other specs.<br>\n",
    "    <br>\n",
    "    we thought to our selves what affects on the price the most. is it the brand? is it the graphics card? or is it a simple thing like the type of RAM or hard drive? \n",
    "    <br>\n",
    "    Also, could we predict the price of a laptop based on it's specs? which elemnts are critical for that prediction?\n",
    "    <br>\n",
    "    To answer the following questions we set a goal to our research-  to see if we can build a model that will predict the price of a laptop based of it's specs and also to figure out which of the specs affect the price the most.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "<h2 id=\"imports\">Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import time\n",
    "from time import sleep\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_acquisition\"></a>\n",
    "<h2>Data Acquisition</h2>\n",
    "<p>Considering our options of data acquisition sources, we decided to look for the biggest electronics comparsion websites and scrape data which we thought will be helpful and save it as a dataframe.\n",
    "\n",
    "The top options we found were:</p>\n",
    "<ul>\n",
    "   <li> <a href=\"https://zap.com/\"target=\"_blank\">Zap</a></li>\n",
    "    <li><a href=\"https://amazon.com/\"target=\"_blank\">Amazon</a></li>\n",
    "    <li><a href=\"https://www.ret.co.il/\"target=\"_blank\">Ret</a></li>\n",
    "    <li><a href=\"https://www.gadgetsnow.com/\"target=\"_blank\">gadgetsnow</a></li>\n",
    "    <li><a href=\"https://www.wisebuy.co.il/\"target=\"_blank\">wisebuy</a></li>\n",
    "</ul>\n",
    "\n",
    "<p>After a punctual search in these websites for relevant data and great community scale, we have decided to move on with Zap due to the popularity of the site and amount of features provided for each laptop.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scraping_challanges'></a>\n",
    "<h3>Scraping challanges</h3>\n",
    "<p>While we trying our first attempts to scrape the data from good reads we found out the following issues:<br>\n",
    "    <ul>\n",
    "       <li> When sending too many requests in a short period of time, the website started to slow down and the scraping took a long period of time.</li>\n",
    "        <li>The default user agent used by requests was python: \"requests/2.26.0\" and we were blocked by Zap after a few requests.</li>\n",
    "     </ul>\n",
    "</p>\n",
    "<p> The way we dealed with those issues was by using sleep and a custom header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "Some util functions to help scrape the data:\n",
    "</h3>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>sleepms- function made in order to create a delay so the amount of requests sends to the server in parralel will not affect the scraping performance and other users experience in the website</li>\n",
    "        <li>random_wait- function which is used in order to create random delay so our scraping methodology will not be detected by the servers.</li>\n",
    "       <li> get_fake_user_agent- function which provides us with a non suspectable user agent.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleepms(milliseconds: int):\n",
    "    seconds = 0.001 * milliseconds\n",
    "    sleep(seconds)\n",
    "    \n",
    "def random_wait():\n",
    "    start = time.time()\n",
    "    sleepms(randint(1, 250))\n",
    "\n",
    "def get_fake_user_agent():\n",
    "    return {\"User-Agent\":\"PostmanRuntime/7.29.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_number(string: str,replaces: List [str]):\n",
    "    #to be continued\n",
    "    # return number_converter(\"\".join(string.split(\",\")))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scraping_process'></a>\n",
    "<h3>Scraping Process</h3>\n",
    "<p>\n",
    "At first glance at the website we thought to ourselves, just how exactly we want to scrape the data?<br>\n",
    "So went in to the website and started exploring the website in order to find the best route for our scraping proccess.\n",
    "</p>\n",
    "<p>\n",
    "We could clearly see that our best option to get maximum data was in the extra details page which contains huge amount of data.<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/extra_details_page.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</p>\n",
    "At this point we asked ourselves what is the exact data we wish to scrape?<br>\n",
    "\n",
    "we decided to go with the following and rename them for the rest of the process:\n",
    "<ul>\n",
    "    <li>יצרן - Brand</li>\n",
    "    <li>תאריך כניסה לזאפ - Zap Enter Date</li>\n",
    "    <li>התאמה לגיימינג - Gaming Compability</li>\n",
    "    <li>מערכת הפעלה - Operating System</li>\n",
    "    <li>תצורת 2 ב- 1 - Two in One</li>\n",
    "    <li>משקל - Weight</li>\n",
    "    <li>סדרה - Series</li>\n",
    "    <li>סוג מעבד - CPU type</li>\n",
    "    <li>נפח זיכרון RAM - RAM Capacity</li>\n",
    "    <li>מהירות מעבד - CPU speed</li>\n",
    "    <li>דגם מעבד - CPU modal</li>\n",
    "    <li>דור מעבד - CPU generation</li>\n",
    "    <li>כונן קשיח - Hard drive capacity</li>\n",
    "    <li>מהירות כונן קשיח - Hard drive Type</li>\n",
    "    <li>כונן אופטי - Optical Drive</li>\n",
    "    <li>גודל מסך - Screen Size</li>\n",
    "    <li>רזולוצית מסך - Screen Resolution</li>\n",
    "    <li>סוג מסך - Screen Type</li>\n",
    "    <li>קצב ריענון תצוגה - FPS</li>\n",
    "    <li>מסך מגע - Touch Screen</li>\n",
    "    <li>כרטיס מסך - GPU</li>\n",
    "    <li>מצלמת רשת - Web camera</li>\n",
    "    <li>אמצעי אבטחה - Secutiry</li>\n",
    "    <li>חיבורים - Connectors</li>\n",
    "    <li>רשת אלחוטית - Wifi modal</li>\n",
    "    <li>מודם סלולרי - Cellular modem</li>\n",
    "    <li>מחיר מינימלי - Min price</li>\n",
    "    <li>מחיר מקסימלי - Max price</li>\n",
    "</ul>\n",
    "Also, we decided to save the page number in which Zap decided to place the laptop, we figured it has a meaning we could use:\n",
    "<ul>\n",
    "    <li>Page Number</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_data(link): \n",
    "    page = requests.get(link, headers = get_fake_user_agent())\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    links = soup.findAll ('div', {'class' : 'detailsRow'}, limit=None)\n",
    "    titles = ['מחיר מינימלי','מחיר מקסימלי']\n",
    "    spans = soup.find('div','PricesTxt').findAll('span') \n",
    "    if len(spans) == 2:\n",
    "        values = [ spans[1].text, spans[0].text ]\n",
    "    elif len(spans) == 1: \n",
    "        values = [ spans[0].text, spans[0].text ]\n",
    "    else:\n",
    "        values = [ None,None ]\n",
    "    \n",
    "    for i in range(0, len(links)):\n",
    "        titles.append(links[i].find('div','detailsRowTitle').text.replace('?','').strip())\n",
    "        values.append(links[i].find('div','detailsRowTxt').text.strip())\n",
    "\n",
    "\n",
    "    return dict(zip(titles,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_names(df):\n",
    "    translated_df = pd.DataFrame()\n",
    "    translation = {\n",
    "   \"יצרן\":\"Brand\",\n",
    "   \"תאריך כניסה לזאפ\":\"Zap Enter Date\",\n",
    "   \"התאמה לגיימינג\":\"Gaming Compability\",\n",
    "   \"מערכת הפעלה\":\"Operating System\",\n",
    "   \"תצורת ‎ 2 in 1\":\"Two in One\",\n",
    "   \"משקל\":\"Weight\",\n",
    "   \"סדרה\":\"Series\",\n",
    "   \"סוג מעבד\":\"CPU type\",\n",
    "  \"פח זיכרון RAM\":\"RAM Capacity\",\n",
    "   \"מהירות מעבד\":\"CPU speed\",\n",
    "   \"דגם מעבד\":\"CPU modal\",\n",
    "   \"דור מעבד\":\"CPU generation\",\n",
    "   \"כונן קשיח\":\"Hard drive capacity\",\n",
    "   \"מהירות כונן קשיח\":\"Hard drive Type\",\n",
    "  \"כונן אופטי\":\"Optical Drive\",\n",
    "   \"גודל מסך\":\"Screen Size\",\n",
    "   \"רזולוצית מסך\":\"Screen Resolution\",\n",
    "   \"סוג מסך\":\"Screen Type\",\n",
    "   \"קצב ריענון תצוגה\":\"FPS\",\n",
    "   \"מסך מגע\":\"Touch Screen\",\n",
    "   \"כרטיס מסך\":\"GPU\",\n",
    "   \"מצלמת רשת\":\"Web camera\",\n",
    "   \"אמצעי אבטחה\":\"Secutiry\",\n",
    "   \"חיבורים\":\"Connectors\",\n",
    "   \"רשת אלחוטית\":\"Wifi modal\",\n",
    "   \"מודם סלולרי\":\"Cellular modem\",\n",
    "   \"מחיר מינימלי\":\"Min price\",\n",
    "   \"מחיר מקסימלי\":\"Max price\"}\n",
    "\n",
    "    for key, value in translation.items():\n",
    "        translated_df[value] = df[key]\n",
    "\n",
    "    return translated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>First Crawl Conclusion - Continue</h3>\n",
    "<p>\n",
    "In addition, after further investigating our scrape target website, we see that all genres have up to 25 pages (1250 books) to browse (AT MAX).\n",
    "Therefore, We are going to run the scrape flow again so we fetch every book we are able to (by selected genres), but this time change the genreSpider class to support offset, so it won't re-read the same books again, AND we will append the books this time to the existing parquet file, instead of overwriting them... other than that, same approach.\n",
    "NOTE: original Class as been updated.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        Same Target Genres, <strong>Altough, only the ones which have any books left to scrape. (the genres which has 1000 books in their df (prev max)</strong>\n",
    "    </li>\n",
    "    <li>\n",
    "        same multi-threades approach\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zap_data_crawl_all(BASE_URL,filename,page):\n",
    "    index = page\n",
    "    df = pd.DataFrame()\n",
    "    url = f'{BASE_URL}models.aspx?sog=c:\"pclaptop&pageinfo={index}'\n",
    "    page = requests.get(url,headers = get_fake_user_agent())\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    while soup.select_one('.selectedNumBtn') and soup.select_one('.selectedNumBtn').text == str(index):\n",
    "        print(f'page number {index}')\n",
    "        links = soup.findAll ('div', {'class' : 'MoreInfo'}, limit=None)   \n",
    "        for i in range(0, len(links)):\n",
    "            link = BASE_URL + links[i].find('a')[\"href\"]\n",
    "            cd = crawl_data(link)\n",
    "            cd['מספר עמוד'] = index\n",
    "            df = pd.concat([df,pd.DataFrame([cd])])\n",
    "            \n",
    "        index += 1\n",
    "        random_wait()\n",
    "        url = f'{BASE_URL}models.aspx?sog=c:\"pclaptop&pageinfo={index}'\n",
    "        page = requests.get(url,headers = get_fake_user_agent())\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    translated_df = translate_names(df)\n",
    "    translated_df.to_csv(filename, index=False ,encoding = 'utf:\"8:\"sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL =\"https://www.zap.co.il/\"\n",
    "filename = 'test.csv'\n",
    "page = 1\n",
    "zap_data_crawl_all(BASE_URL, filename, page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
